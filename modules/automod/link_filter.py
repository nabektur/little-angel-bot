import re
import unicodedata
import urllib.parse

from cache     import AsyncTTL
from rapidfuzz import fuzz

VARIATION_SELECTOR_RE = re.compile(r"[\uFE0F]")
ZERO_WIDTH_RE = re.compile(r"[\u200B-\u200F\uFEFF\u2060]")

# emoji-–±—É–∫–≤ -> ASCII
EMOJI_ASCII_MAP = {
    "üÖ∞Ô∏è": "a", "üÖ±Ô∏è": "b", "üÖæÔ∏è": "o", "üÖøÔ∏è": "p",
    "‚ìÇÔ∏è": "m", "‚ÑπÔ∏è": "i", "‚ùå": "x", "‚≠ï": "o",
}

# üá¶ -> a
REGIONAL_INDICATOR_MAP = {
    chr(code): chr(ord('a') + (code - 0x1F1E6))
    for code in range(0x1F1E6, 0x1F1FF + 1)
}

# –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
HOMOGLYPHS = {
    "–∞": "a", "–ê": "a",
    "–µ": "e", "–ï": "e", "—ë": "e", "–Å": "e",
    "–æ": "o", "–û": "o",
    "—Ä": "p", "–†": "p",
    "—Å": "c", "–°": "c",
    "—Ö": "x", "–•": "x",
    "—É": "y", "–£": "y",
    "–∫": "k", "–ö": "k",
    "–º": "m", "–ú": "m",
    "—Ç": "t", "–¢": "t",
    "–≤": "b", "–í": "b",
    "–Ω": "h", "–ù": "h",
    "–¥": "d", "–î": "d",
    "–≥": "g", "–ì": "g",
    "–±": "b", "–ë": "b",
    "—ñ": "i", "–Ü": "i",
    # –¶–∏—Ñ—Ä—ã –∏ —Å–∏–º–≤–æ–ª—ã
    "0": "o",
    "1": "l",
    "3": "e",
}

ENCLOSED_ALPHANUM_MAP = {
    "üÑ∞": "a","üÑ±": "b","üÑ≤": "c","üÑ≥": "d","üÑ¥": "e",
    "üÑµ": "f","üÑ∂": "g","üÑ∑": "h","üÑ∏": "i","üÑπ": "j",
    "üÑ∫": "k","üÑª": "l","üÑº": "m","üÑΩ": "n","üÑæ": "o",
    "üÑø": "p","üÖÄ": "q","üÖÅ": "r","üÖÇ": "s","üÖÉ": "t",
    "üÖÑ": "u","üÖÖ": "v","üÖÜ": "w","üÖá": "x","üÖà": "y",
    "üÖâ": "z",
    "üÖê": "a","üÖë": "b","üÖí": "c","üÖì": "d","üÖî": "e",
    "üÖï": "f","üÖñ": "g","üÖó": "h","üÖò": "i","üÖô": "j",
    "üÖö": "k","üÖõ": "l","üÖú": "m","üÖù": "n","üÖû": "o",
    "üÖü": "p","üÖ†": "q","üÖ°": "r","üÖ¢": "s","üÖ£": "t",
    "üÖ§": "u","üÖ•": "v","üÖ¶": "w","üÖß": "x","üÖ®": "y",
    "üÖ©": "z",
    "üÜä": "j","üÜã": "k","üÜå": "l","üÜç": "m","üÜé": "ab",
    "üÜè": "k","üÜê": "p","üÜë": "cl","üÜí": "cool",
    "üÜì": "free","üÜî": "id","üÜï": "new","üÜñ": "ng",
    "üÜó": "ok","üÜò": "sos","üÜô": "up",
    "üÜö": "vs","üÜõ": "b","üÜú": "m","üÜù": "n",
    "üÜû": "o","üÜü": "p","üÜ†": "q","üÜ°": "p",
    "üÜ¢": "s","üÜ£": "t","üÜ§": "u","üÜ•": "v",
    "üÜ¶": "w","üÜß": "x","üÜ®": "h","üÜ©": "i",
    "üÜ™": "j","üÜ´": "k","üÜ¨": "l","üÜ≠": "m",
    "üÜÆ": "n","üÜØ": "o",
}

FANCY_MAP = {
    **{chr(i): chr(i - 0xFEE0).lower() for i in range(0xFF21, 0xFF3B)},
    **{chr(i): chr(i - 0xFEE0).lower() for i in range(0xFF41, 0xFF5B)},
    **{chr(0x1D400 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D41A + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D434 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D44E + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D468 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D482 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D49C + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,7,11,12,17,18]},
    **{chr(0x1D4B6 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D4D0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D4EA + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D504 + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,18,23]},
    **{chr(0x1D51E + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D538 + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,17]},
    **{chr(0x1D552 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5A0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5BA + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5D4 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5EE + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D608 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D622 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D63C + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D656 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D670 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D68A + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x24B6 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x24D0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F150 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F130 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F170 + i): chr(ord('a') + i) for i in range(26)},
}

_COMBINED_MAP = {}
_COMBINED_MAP.update(EMOJI_ASCII_MAP)
_COMBINED_MAP.update(REGIONAL_INDICATOR_MAP)
_COMBINED_MAP.update(ENCLOSED_ALPHANUM_MAP)
_COMBINED_MAP.update(HOMOGLYPHS)
_COMBINED_MAP.update(FANCY_MAP)

async def _char_to_ascii(ch: str) -> str:
    if VARIATION_SELECTOR_RE.match(ch):
        return ""
    if ZERO_WIDTH_RE.match(ch):
        return ""
    if ch in _COMBINED_MAP:
        return _COMBINED_MAP[ch]

    code = ord(ch)
    if 0x1F1E6 <= code <= 0x1F1FF:
        return chr(ord("a") + (code - 0x1F1E6))

    decomp = unicodedata.normalize("NFKD", ch)
    if decomp:
        base = decomp[0]
        if ('A' <= base <= 'Z') or ('a' <= base <= 'z'):
            return base.lower()

    if ch.isdigit():
        return ch

    if ch in " \t\r\n./\\|_‚Ä¢¬∑-:":
        return " "

    try:
        name = unicodedata.name(ch)
    except ValueError:
        name = ""

    if name:
        nm = name.upper().split()
        for token in nm:
            if len(token) == 1 and 'A' <= token <= 'Z':
                return token.lower()

    return " "

async def normalize_and_compact(raw_text: str) -> str:
    try:
        text = urllib.parse.unquote(raw_text)
    except Exception:
        text = raw_text

    text = unicodedata.normalize("NFKC", text)

    out = []
    for ch in text:
        out.append(await _char_to_ascii(ch))

    collapsed = "".join(out)
    collapsed = re.sub(r"\s+", " ", collapsed).strip()
    compact = re.sub(r"[^a-z0-9]", "", collapsed.lower())
    return compact

async def looks_like_discord(word: str, threshold=85):
    """–ü–æ–≤—ã—à–µ–Ω –ø–æ—Ä–æ–≥ —Å 70 –¥–æ 85 –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π"""
    if len(word) < 6:
        return False
    score = fuzz.partial_ratio("discord", word)
    return score >= threshold

def extract_markdown_links(text: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏ [—Ç–µ–∫—Å—Ç](url)"""
    return re.findall(r'\[([^\]]+)\]\(([^\)]+)\)', text)

def is_natural_word_context(text: str, match_pos: int, match_len: int) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å–ª–æ–≤–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True, –µ—Å–ª–∏ —ç—Ç–æ —á–∞—Å—Ç—å –æ–±—ã—á–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    """
    # –ë–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    start = max(0, match_pos - 20)
    end = min(len(text), match_pos + match_len + 20)
    context = text[start:end].lower()
    
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    natural_indicators = [
        # –†—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ —Ä—è–¥–æ–º
        r'[–∞-—è—ë]{3,}',
        # –ó–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        r'[,;:!?]',
        # –¢–∏–ø–∏—á–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–æ—é–∑—ã
        r'\b(–∏|–≤|–Ω–∞|—Å|—á—Ç–æ|–∫–∞–∫|—ç—Ç–æ|–¥–ª—è|–æ—Ç|–ø–æ|–Ω–æ|–∞|–∏–ª–∏)\b',
    ]
    
    for pattern in natural_indicators:
        if re.search(pattern, context):
            return True
    
    return False

def extract_spaced_patterns(text: str, compact: str):
    """
    –ò—â–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–Ω–æ —Ä–∞–∑–Ω–µ—Å–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∏–¥–∞ 't . m e' –∏–ª–∏ 'd i s c o r d . g g'
    """
    findings = []
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
    patterns = [
        # t.me —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        (r't[\s\.\-_‚Ä¢]{0,3}\.[\s\.\-_‚Ä¢]{0,3}m[\s\.\-_‚Ä¢]{0,3}e[\s\.\-_‚Ä¢]{0,3}/[\s\.\-_‚Ä¢]{0,3}\w+', "t.me"),
        (r't[\s\.\-_‚Ä¢]{1,3}m[\s\.\-_‚Ä¢]{1,3}e[\s\.\-_‚Ä¢]{0,3}/[\s\.\-_‚Ä¢]{0,3}\w+', "t.me"),
        
        # discord.gg —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        (r'd[\s\.\-_‚Ä¢]{0,2}i[\s\.\-_‚Ä¢]{0,2}s[\s\.\-_‚Ä¢]{0,2}c[\s\.\-_‚Ä¢]{0,2}o[\s\.\-_‚Ä¢]{0,2}r[\s\.\-_‚Ä¢]{0,2}d[\s\.\-_‚Ä¢]{0,3}\.[\s\.\-_‚Ä¢]{0,3}g[\s\.\-_‚Ä¢]{0,3}g', "discord.gg"),
        (r'd[\s\.\-_‚Ä¢]{0,2}i[\s\.\-_‚Ä¢]{0,2}s[\s\.\-_‚Ä¢]{0,2}c[\s\.\-_‚Ä¢]{0,2}[\s\.\-_‚Ä¢]{0,2}r[\s\.\-_‚Ä¢]{0,2}d[\s\.\-_‚Ä¢]{0,3}\.[\s\.\-_‚Ä¢]{0,3}g[\s\.\-_‚Ä¢]{0,3}g', "discord.gg"),
        
        # discordapp —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏  
        (r'd[\s\.\-_‚Ä¢]{0,2}i[\s\.\-_‚Ä¢]{0,2}s[\s\.\-_‚Ä¢]{0,2}c[\s\.\-_‚Ä¢]{0,2}o[\s\.\-_‚Ä¢]{0,2}r[\s\.\-_‚Ä¢]{0,2}d[\s\.\-_‚Ä¢]{0,2}a[\s\.\-_‚Ä¢]{0,2}p[\s\.\-_‚Ä¢]{0,2}p', "discordapp.com"),
        
        # telegram —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
        (r't[\s\.\-_‚Ä¢]{0,2}e[\s\.\-_‚Ä¢]{0,2}l[\s\.\-_‚Ä¢]{0,2}e[\s\.\-_‚Ä¢]{0,2}g[\s\.\-_‚Ä¢]{0,2}r[\s\.\-_‚Ä¢]{0,2}a[\s\.\-_‚Ä¢]{0,2}m[\s\.\-_‚Ä¢]{0,3}\.[\s\.\-_‚Ä¢]{0,3}(me|org)', "telegram"),
    ]
    
    text_lower = text.lower()
    
    for pattern, label in patterns:
        matches = re.finditer(pattern, text_lower)
        for match in matches:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if not is_natural_word_context(text, match.start(), len(match.group())):
                findings.append((label, match.group()))
    
    return findings

def extract_possible_domains(text: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_no_spaces = text.replace(" ", "")
    candidates = []

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã —Å —Ç–æ—á–∫–æ–π
    dom1 = re.findall(r"([a-zA-Z0-9]+)\.([a-zA-Z]{2,6})\b", text_no_spaces)
    for a, b in dom1:
        candidates.append(a + "." + b)

    # –°–∫–ª–µ–µ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã - –±–æ–ª–µ–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã–π –ø–æ–¥—Ö–æ–¥
    dom2 = re.findall(r"([a-zA-Z0-9]{6,})(gg|com|app)\b", text_no_spaces)
    for a, b in dom2:
        candidates.append(a + b)

    return candidates

@AsyncTTL(time_to_live=600, maxsize=20000)
async def detect_links(raw_text: str):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏ –∏–ª–∏ None
    """
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –±–µ–∑ —è–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if len(raw_text) < 8:
        return None
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
    compact = await normalize_and_compact(raw_text)
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω–µ—Å–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–æ–Ω–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–µ–µ)
    spaced_findings = extract_spaced_patterns(raw_text, compact)
    if spaced_findings:
        label, matched = spaced_findings[0]
        return f"{label} (–∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞: {matched})"
    
    # –®–∞–≥ 1: –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ markdown
    markdown_links = extract_markdown_links(raw_text)
    all_urls_to_check = [raw_text]
    
    for link_text, url in markdown_links:
        all_urls_to_check.append(url)
        all_urls_to_check.append(link_text)
    
    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
    for text_fragment in all_urls_to_check:
        result = await _check_single_fragment(text_fragment, raw_text, compact)
        if result:
            return result
    
    return None


async def _check_single_fragment(text_fragment: str, original_text: str, compact: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫"""
    
    # –ï—Å–ª–∏ compact –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –≤—ã—á–∏—Å–ª—è–µ–º
    if not compact:
        compact = await normalize_and_compact(text_fragment)

    if "tme" in compact and ("t.me" in text_fragment.lower()):
        return "t.me"
    
    text_lower = text_fragment.replace(" ", "").lower()
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    if len(compact) < 5:
        return None
    
    # --- Discord ---
    # –Ø–≤–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    if "discordgg" in compact:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å –æ–±—ã—á–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        match_pos = text_fragment.lower().find("discord")
        if match_pos != -1:
            if is_natural_word_context(text_fragment, match_pos, 7):
                return None
        return "discord.gg"
    
    if "discordcom" in compact:
        if "/channels/" not in text_lower:
            match_pos = text_fragment.lower().find("discord")
            if match_pos != -1:
                if is_natural_word_context(text_fragment, match_pos, 7):
                    return None
            return "discord.com"
    
    if "discordappcom" in compact:
        if not any(x in original_text for x in ["https://cdn.discordapp.com", "https://media.discordapp.net", "https://images-ext-1.discordapp.net"]):
            return "discordapp.com"
        elif "invite" in compact:
            return "discordapp.com"
    
    # --- Telegram ---
    if "telegramme" in compact or "telegramorg" in compact:
        return "telegram.me" if "telegramme" in compact else "telegram.org"
    
    # t.me - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if "tme" in compact:
        # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        tme_patterns = [r't\.me/', r't\s*\.\s*me/', r'tme/']
        for pattern in tme_patterns:
            if re.search(pattern, text_lower):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                match = re.search(pattern, text_lower)
                if match and not is_natural_word_context(text_fragment, match.start(), len(match.group())):
                    return "t.me"
    
    # --- –î–æ–º–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ---
    candidates = extract_possible_domains(compact)
    
    for cand in candidates:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        if len(cand) < 8:
            continue
            
        left = cand.split(".")[0].replace("gg","").replace("com","").replace("app","")
        
        if await looks_like_discord(left):
            # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ "discord"
            if left == "discord":
                continue
            
            # –ò—Å–∫–ª—é—á–∞–µ–º CDN
            if any(x in cand for x in ["imagesext1discordapp", "mediadiscordapp", "cdndiscordapp"]):
                if "invite" not in compact:
                    continue
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            if "/channels/" in text_lower:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            match_pos = text_fragment.lower().find(left)
            if match_pos != -1:
                if is_natural_word_context(text_fragment, match_pos, len(left)):
                    continue
            
            return f"–ü–æ—Ö–æ–∂–µ –Ω–∞ —Å—Å—ã–ª–∫—É –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è –≤ Discord —Å–µ—Ä–≤–µ—Ä ({cand})"
    
    return None