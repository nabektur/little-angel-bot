import re
import unicodedata
import urllib.parse

from cache     import AsyncLRU
from rapidfuzz import fuzz

VARIATION_SELECTOR_RE = re.compile(r"[\uFE0F]")
ZERO_WIDTH_RE = re.compile(r"[\u200B-\u200F\uFEFF\u2060]")

# emoji-–±—É–∫–≤ -> ASCII
EMOJI_ASCII_MAP = {
    "üÖ∞Ô∏è": "a", "üÖ±Ô∏è": "b", "üÖæÔ∏è": "o", "üÖøÔ∏è": "p",
    "‚ìÇÔ∏è": "m", "‚ÑπÔ∏è": "i", "‚ùå": "x", "‚≠ï": "o",
}

# üá¶ -> a
REGIONAL_INDICATOR_MAP = {
    chr(code): chr(ord('a') + (code - 0x1F1E6))
    for code in range(0x1F1E6, 0x1F1FF + 1)
}

# –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ -> –ª–∞—Ç–∏–Ω–∏—Ü–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
HOMOGLYPHS = {
    "–∞": "a", "–ê": "a",
    "–µ": "e", "–ï": "e", "—ë": "e", "–Å": "e",
    "–æ": "o", "–û": "o",
    "—Ä": "p", "–†": "p",
    "—Å": "c", "–°": "c",
    "—Ö": "x", "–•": "x",
    "—É": "y", "–£": "y",
    "–∫": "k", "–ö": "k",
    "–º": "m", "–ú": "m",
    "—Ç": "t", "–¢": "t",
    "–≤": "b", "–í": "b",
    "–Ω": "h", "–ù": "h",
    "–¥": "d", "–î": "d",
    "–≥": "g", "–ì": "g",
    "–±": "b", "–ë": "b",
    "—ñ": "i", "–Ü": "i",
    # –¶–∏—Ñ—Ä—ã –∏ —Å–∏–º–≤–æ–ª—ã
    "0": "o",
    "1": "l",
    "3": "e",
}

ENCLOSED_ALPHANUM_MAP = {
    "üÑ∞": "a","üÑ±": "b","üÑ≤": "c","üÑ≥": "d","üÑ¥": "e",
    "üÑµ": "f","üÑ∂": "g","üÑ∑": "h","üÑ∏": "i","üÑπ": "j",
    "üÑ∫": "k","üÑª": "l","üÑº": "m","üÑΩ": "n","üÑæ": "o",
    "üÑø": "p","üÖÄ": "q","üÖÅ": "r","üÖÇ": "s","üÖÉ": "t",
    "üÖÑ": "u","üÖÖ": "v","üÖÜ": "w","üÖá": "x","üÖà": "y",
    "üÖâ": "z",
    "üÖê": "a","üÖë": "b","üÖí": "c","üÖì": "d","üÖî": "e",
    "üÖï": "f","üÖñ": "g","üÖó": "h","üÖò": "i","üÖô": "j",
    "üÖö": "k","üÖõ": "l","üÖú": "m","üÖù": "n","üÖû": "o",
    "üÖü": "p","üÖ†": "q","üÖ°": "r","üÖ¢": "s","üÖ£": "t",
    "üÖ§": "u","üÖ•": "v","üÖ¶": "w","üÖß": "x","üÖ®": "y",
    "üÖ©": "z",
    "üÜä": "j","üÜã": "k","üÜå": "l","üÜç": "m","üÜé": "ab",
    "üÜè": "k","üÜê": "p","üÜë": "cl","üÜí": "cool",
    "üÜì": "free","üÜî": "id","üÜï": "new","üÜñ": "ng",
    "üÜó": "ok","üÜò": "sos","üÜô": "up",
    "üÜö": "vs","üÜõ": "b","üÜú": "m","üÜù": "n",
    "üÜû": "o","üÜü": "p","üÜ†": "q","üÜ°": "p",
    "üÜ¢": "s","üÜ£": "t","üÜ§": "u","üÜ•": "v",
    "üÜ¶": "w","üÜß": "x","üÜ®": "h","üÜ©": "i",
    "üÜ™": "j","üÜ´": "k","üÜ¨": "l","üÜ≠": "m",
    "üÜÆ": "n","üÜØ": "o",
}

FANCY_MAP = {
    **{chr(i): chr(i - 0xFEE0).lower() for i in range(0xFF21, 0xFF3B)},
    **{chr(i): chr(i - 0xFEE0).lower() for i in range(0xFF41, 0xFF5B)},
    **{chr(0x1D400 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D41A + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D434 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D44E + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D468 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D482 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D49C + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,7,11,12,17,18]},
    **{chr(0x1D4B6 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D4D0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D4EA + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D504 + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,18,23]},
    **{chr(0x1D51E + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D538 + i): chr(ord('a') + i) for i in range(26) if i not in [1,4,17]},
    **{chr(0x1D552 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5A0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5BA + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5D4 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D5EE + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D608 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D622 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D63C + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D656 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D670 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1D68A + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x24B6 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x24D0 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F150 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F130 + i): chr(ord('a') + i) for i in range(26)},
    **{chr(0x1F170 + i): chr(ord('a') + i) for i in range(26)},
}

_COMBINED_MAP = {}
_COMBINED_MAP.update(EMOJI_ASCII_MAP)
_COMBINED_MAP.update(REGIONAL_INDICATOR_MAP)
_COMBINED_MAP.update(ENCLOSED_ALPHANUM_MAP)
_COMBINED_MAP.update(HOMOGLYPHS)
_COMBINED_MAP.update(FANCY_MAP)

async def _char_to_ascii(ch: str) -> str:
    if VARIATION_SELECTOR_RE.match(ch):
        return ""
    if ZERO_WIDTH_RE.match(ch):
        return ""
    if ch in _COMBINED_MAP:
        return _COMBINED_MAP[ch]

    code = ord(ch)
    if 0x1F1E6 <= code <= 0x1F1FF:
        return chr(ord("a") + (code - 0x1F1E6))

    decomp = unicodedata.normalize("NFKD", ch)
    if decomp:
        base = decomp[0]
        if ('A' <= base <= 'Z') or ('a' <= base <= 'z'):
            return base.lower()

    if ch.isdigit():
        return ch

    if ch in " \t\r\n./\\|_‚Ä¢¬∑-:":
        return " "

    try:
        name = unicodedata.name(ch)
    except ValueError:
        name = ""

    if name:
        nm = name.upper().split()
        for token in nm:
            if len(token) == 1 and 'A' <= token <= 'Z':
                return token.lower()

    return " "

async def normalize_and_compact(raw_text: str) -> str:
    try:
        text = urllib.parse.unquote(raw_text)
    except Exception:
        text = raw_text

    text = unicodedata.normalize("NFKC", text)

    out = []
    for ch in text:
        out.append(await _char_to_ascii(ch))

    collapsed = "".join(out)
    collapsed = re.sub(r"\s+", " ", collapsed).strip()
    compact = re.sub(r"[^a-z0-9]", "", collapsed.lower())
    return compact

async def looks_like_discord(word: str, threshold=85):
    """–ü–æ–≤—ã—à–µ–Ω –ø–æ—Ä–æ–≥ —Å 70 –¥–æ 85 –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π"""
    if len(word) < 6:  # –£–≤–µ–ª–∏—á–µ–Ω –º–∏–Ω–∏–º—É–º —Å 5 –¥–æ 6
        return False
    score = fuzz.partial_ratio("discord", word)
    return score >= threshold

def extract_markdown_links(text: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ markdown-—Ä–∞–∑–º–µ—Ç–∫–∏ [—Ç–µ–∫—Å—Ç](url)"""
    return re.findall(r'\[([^\]]+)\]\(([^\)]+)\)', text)

def has_url_markers(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —è–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ URL (http, //, —Ç–æ—á–∫–∞ —Å –¥–æ–º–µ–Ω–æ–º)"""
    text_lower = text.lower()
    
    # –Ø–≤–Ω—ã–µ –ø—Ä–æ—Ç–æ–∫–æ–ª—ã
    if "http://" in text_lower or "https://" in text_lower:
        return True
    
    # –î–≤–æ–π–Ω–æ–π —Å–ª—ç—à –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ –≤–æ–∫—Ä—É–≥
    if re.search(r'\S//\S', text):
        return True
    
    # –î–æ–º–µ–Ω —Å —Ç–æ—á–∫–æ–π –∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–º TLD
    if re.search(r'\w+\.(com|gg|net|org|app|io|me|xyz|ru|lv)\b', text_lower):
        return True
    
    return False

def extract_possible_domains(text: str):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    text_no_spaces = text.replace(" ", "")
    candidates = []

    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã —Å —Ç–æ—á–∫–æ–π
    dom1 = re.findall(r"([a-zA-Z0-9]+)\.([a-zA-Z]{2,6})\b", text_no_spaces)
    for a, b in dom1:
        candidates.append(a + "." + b)

    # –°–∫–ª–µ–µ–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã URL)
    if has_url_markers(text):
        dom2 = re.findall(r"([a-zA-Z0-9]{5,})(gg|com|app|net)", text_no_spaces)
        for a, b in dom2:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å –æ–±—ã—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            if len(a) > 10:  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ —Å–ª–æ–≤–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ
                candidates.append(a + b)

    return candidates

@AsyncLRU(maxsize=5000)
async def detect_links(raw_text: str):
    """
    –î–µ—Ç–µ–∫—Ç–∏—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π —Å—Å—ã–ª–∫–∏ –∏–ª–∏ None
    """
    
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –∏ –±–µ–∑ URL-–º–∞—Ä–∫–µ—Ä–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if len(raw_text) < 10 and not has_url_markers(raw_text):
        return None
    
    # –®–∞–≥ 1: –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏–∑ markdown
    markdown_links = extract_markdown_links(raw_text)
    all_urls_to_check = [raw_text]
    
    for link_text, url in markdown_links:
        all_urls_to_check.append(url)
        all_urls_to_check.append(link_text)
    
    # –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
    for text_fragment in all_urls_to_check:
        result = await _check_single_fragment(text_fragment, raw_text)
        if result:
            return result
    
    return None

async def _check_single_fragment(text_fragment: str, original_text: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–∏–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫"""
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
    compact = await normalize_and_compact(text_fragment)
    text_lower = text_fragment.replace(" ", "").lower()
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
    if len(compact) < 5:
        return None
    
    # --- Discord ---
    # –Ø–≤–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    if "discordgg" in compact or "discordcom" in compact or "discordappcom" in compact:
        if "discordgg" in compact:
            return "discord.gg"
        if "discordcom" in compact:
            if "/channels/" not in text_lower:
                return "discord.com"
        if "discordappcom" in compact:
            if not any(x in original_text for x in ["https://cdn.discordapp.com", "https://media.discordapp.net", "https://images-ext-1.discordapp.net"]):
                return "discordapp.com"
            elif "invite" in compact:
                return "discordapp.com"
    
    # --- Telegram ---
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å—Å—ã–ª–∫–∏
    if has_url_markers(text_fragment):
        if "telegramme" in compact or "telegramorg" in compact:
            return "telegram.me" if "telegramme" in compact else "telegram.org"
        
        # t.me —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª—ç—à –∏–ª–∏ —Ç–æ—á–∫–∞ —Ä—è–¥–æ–º
        if re.search(r't\.me/\w+', text_lower) or re.search(r't\.me\s', text_lower):
            return "t.me"
        
        if re.search(r"(telegram\.me|telegram\.org)/", text_lower):
            m = re.search(r"(telegram\.me|telegram\.org)", text_lower)
            return m.group(1)
    
    # --- –î–æ–º–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ---
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ URL
    if not has_url_markers(text_fragment):
        return None
    
    candidates = extract_possible_domains(compact)
    
    for cand in candidates:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã
        if len(cand) < 8:
            continue
            
        left = cand.split(".")[0].replace("gg","").replace("com","").replace("app","")
        
        if await looks_like_discord(left):
            # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ "discord"
            if left == "discord":
                continue
            
            # –ò—Å–∫–ª—é—á–∞–µ–º CDN
            if any(x in cand for x in ["imagesext1discordapp", "mediadiscordapp", "cdndiscordapp"]):
                if "invite" not in compact:
                    continue
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            if "/channels/" in text_lower:
                continue
            
            return f"–ü–æ—Ö–æ–∂–µ –Ω–∞ —Å—Å—ã–ª–∫—É –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏—è –≤ Discord —Å–µ—Ä–≤–µ—Ä ({cand})"
    
    return None